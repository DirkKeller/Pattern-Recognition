{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from labml import lab, tracker, experiment, monit\n",
    "from labml.configs import BaseConfigs, option\n",
    "from labml_helpers.device import DeviceConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Denoising Diffusion Probabilistic Models (DDPM) training\n",
    "summary: >\n",
    "  Training code for\n",
    "  Denoising Diffusion Probabilistic Model.\n",
    "---\n",
    "\n",
    "# [Denoising Diffusion Probabilistic Models (DDPM)](index.html) training\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]\n",
    "(https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.ipynb)\n",
    "\n",
    "This trains a DDPM based model on CelebA HQ dataset. You can find the download instruction in this\n",
    "[discussion on fast.ai](https://forums.fast.ai/t/download-celeba-hq-dataset/45873/3).\n",
    "Save the images inside [`data/celebA` folder](#dataset_path).\n",
    "\n",
    "The paper had used an exponential moving average of the model with a decay of $0.9999$. We have skipped this for\n",
    "simplicity.\n",
    "\n",
    "(obtained from: From: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/ddpm/experiment.py)\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from noise import DenoiseDiffusion\n",
    "from unet import UNet\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Settings for restoring/creating experiment\n",
    "    global MY_UUID\n",
    "    global EXP\n",
    "\n",
    "    LOAD_CHECKPOINT = False # True, False\n",
    "    MY_UUID = 'AbeSaveTesting4' \n",
    "    EXP = 'residual' # 'recurrent', 'residual'\n",
    "\n",
    "    # Create experiment\n",
    "    experiment.create(\n",
    "        name='diffuse',\n",
    "        writers={'screen', 'labml'},\n",
    "        uuid=MY_UUID,\n",
    "    )\n",
    "\n",
    "    # Create configurations\n",
    "    configs = Configs()\n",
    "    print(f'Status: Device is using GPU: {torch.cuda.is_available()}')\n",
    "\n",
    "\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        # Set the model\n",
    "        configs.convolutional_block = EXP\n",
    "\n",
    "        # Set configurations. You can override the defaults by passing the values in the dictionary.\n",
    "        experiment.configs(configs, {\n",
    "            'dataset': 'MNIST',  # 'CIFAR10', 'CelebA' 'MNIST'\n",
    "            'image_channels': 1,  # 3, 3, 1\n",
    "            'epochs': 2,  # 100, 100, 5\n",
    "        })\n",
    "\n",
    "        # Initialize\n",
    "        configs.init()\n",
    "\n",
    "        # Set models for saving and loading\n",
    "        experiment.add_pytorch_models({'eps_model': configs.eps_model})\n",
    "\n",
    "        # Start the experiment\n",
    "        with experiment.start():\n",
    "            configs.run()   \n",
    "\n",
    "    elif LOAD_CHECKPOINT:\n",
    "        my_run_uuid = MY_UUID # Note: set this to the run name you want to load\n",
    "        my_checkpoint_uuid = None # Note: set this to the checkpoint id from that run. Default is -1 I think but might not be.\n",
    "\n",
    "        # Load custom configuration of the training run\n",
    "        configs_dict = experiment.load_configs(my_run_uuid)    \n",
    "\n",
    "        # Set configurations\n",
    "        experiment.configs(configs, configs_dict)\n",
    "\n",
    "        # Set configurations. You can override the defaults by passing the values in the dictionary.\n",
    "        # MAKE SURE THIS MATCHES THE CHECKPOINT YOURE LOADING. But maybe, to get more epochs from a finished run, you increase epochs.\n",
    "        # But that's also untested.\n",
    "        experiment.configs(configs, {\n",
    "            'dataset': 'MNIST',  # 'CIFAR10', 'CelebA' 'MNIST'\n",
    "            'image_channels': 1,  # 3, 3, 1\n",
    "            'epochs': 2,  # 100, 100, 5\n",
    "        })\n",
    "\n",
    "        # Initialize\n",
    "        configs.init()\n",
    "\n",
    "        # Set PyTorch modules for saving and loading\n",
    "        experiment.add_pytorch_models({'eps_model': configs.eps_model})\n",
    "\n",
    "        # Load the experiment from \n",
    "        experiment.load(run_uuid=my_run_uuid, checkpoint=my_checkpoint_uuid) # Note: there's also an optional checkpoint number param (checkpoint=[int])\n",
    "\n",
    "        # Start the experiment (note: not using with experiment.start() when loading)\n",
    "        with experiment.start():\n",
    "            configs.run()   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Configs(BaseConfigs):\n",
    "    \"\"\"\n",
    "    Class for holding configuration parameters for training a DDPM model.\n",
    "\n",
    "    Attributes:\n",
    "        device (torch.device):           Device on which to run the model.\n",
    "        eps_model (UNet):                U-Net model for the function `epsilon_theta`.\n",
    "        diffusion (DenoiseDiffusion):    DDPM algorithm.\n",
    "        image_channels (int):            Number of channels in the image (e.g. 3 for RGB).\n",
    "        image_size (int):                Size of the image.\n",
    "        n_channels (int):                Number of channels in the initial feature map.\n",
    "        channel_multipliers (List[int]): Number of channels at each resolution.\n",
    "        is_attention (List[bool]):       Indicates whether to use attention at each resolution.\n",
    "        convolutional_block (str):       Type of the convolutional block used\n",
    "        schedule_name (str):             Function of the noise schedule\n",
    "        n_steps (int):                   Number of time steps.\n",
    "        batch_size (int):                Batch size.\n",
    "        n_samples (int):                 Number of samples to generate.\n",
    "        learning_rate (float):           Learning rate.\n",
    "        epochs (int):                    Number of training epochs.\n",
    "        dataset (torch.utils.data.Dataset):         Dataset to be used for training.\n",
    "        data_loader (torch.utils.data.DataLoader):  DataLoader for loading the data for training.\n",
    "        optimizer (torch.optim.Adam):               Optimizer for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Device to train the model on.\n",
    "    # [`DeviceConfigs`](https://docs.labml.ai/api/helpers.html#labml_helpers.device.DeviceConfigs)\n",
    "    #  picks up an available CUDA device or defaults to CPU.\n",
    "    device: torch.device = DeviceConfigs()\n",
    "    # Retrieve model information\n",
    "    show = True\n",
    "\n",
    "    # U-Net model for $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n",
    "    eps_model: UNet\n",
    "    # [DDPM algorithm](index.html)\n",
    "    diffusion: DenoiseDiffusion\n",
    "\n",
    "    # Number of channels in the image. $3$ for RGB.\n",
    "    image_channels: int = 3\n",
    "    # Image size\n",
    "    image_size: int = 32\n",
    "    # Number of channels in the initial feature map\n",
    "    n_channels: int = 64  # 64 (Default: Ho et al.; Limit is VRAM)\n",
    "    # The list of channel numbers at each resolution.\n",
    "    # The number of channels is `channel_multipliers[i] * n_channels`\n",
    "    channel_multipliers: List[int] = [1, 2, 2, 4]\n",
    "    # The list of booleans that indicate whether to use attention at each resolution\n",
    "    is_attention: List[int] = [False, False, False, True]\n",
    "    # Convolutional block type used in the UNet blocks. Possible options are 'residual' and 'recurrent'.\n",
    "    convolutional_block = 'residual'\n",
    "\n",
    "    # Defines the noise schedule. Possible options are 'linear' and 'cosine'.\n",
    "    schedule_name: str = 'linear'\n",
    "    # Number of time steps $T$ (with $T$ = 1_000 from Ho et al).\n",
    "    n_steps: int = 1000  # 1000 (Default: Ho et al.)\n",
    "\n",
    "    # Batch size\n",
    "    batch_size: int = 64  # 64 (Default: Ho et al.; Limit is VRAM)\n",
    "    # Number of samples to generate\n",
    "    n_samples: int = 1\n",
    "    # Learning rate\n",
    "    learning_rate: float = 2e-5\n",
    "    # Number of training epochs\n",
    "    epochs: int = 100\n",
    "\n",
    "    # Dataset\n",
    "    dataset: torch.utils.data.Dataset\n",
    "    # Dataloader\n",
    "    data_loader: torch.utils.data.DataLoader\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer: torch.optim.Adam\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Initialize the model, dataset, and optimizer objects.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$ model\n",
    "        self.eps_model = UNet(\n",
    "            image_channels=self.image_channels,\n",
    "            n_channels=self.n_channels,\n",
    "            ch_mults=self.channel_multipliers,\n",
    "            is_attn=self.is_attention,\n",
    "            conv_block=self.convolutional_block\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Create [DDPM class](index.html)\n",
    "        self.diffusion = DenoiseDiffusion(\n",
    "            eps_model=self.eps_model,\n",
    "            n_steps=self.n_steps,\n",
    "            # schedule_name=self.schedule_name,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        # Show the number of params used by the model\n",
    "        if self.show:\n",
    "            pytorch_total_params = sum(p.numel() for p in self.eps_model.parameters())\n",
    "            print(f'The total number of parameters are: {pytorch_total_params}')\n",
    "\n",
    "        # Create dataloader\n",
    "        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n",
    "        # Create optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Image logging\n",
    "        tracker.set_image(\"sample\", True)\n",
    "\n",
    "    def sample(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate samples from a trained Denoising Diffusion Probabilistic Model (DDPM).\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Sample from the noise distribution at the final time step: x_T ~ p(x_T) = N(x_T; 0, I)\n",
    "            x = torch.randn([self.n_samples, self.image_channels, self.image_size, self.image_size],\n",
    "                            device=self.device)\n",
    "\n",
    "            # Remove noise at each time step in reverse order (so remove noise for T steps)\n",
    "            for t_ in monit.iterate('Sample', self.n_steps):\n",
    "                # Get current time step\n",
    "                t = self.n_steps - t_ - 1\n",
    "                # Sample from the noise distribution at the current time step: x_{t-1} ~ p_theta(x_{t-1}|x_t)\n",
    "                x = self.diffusion.p_sample(x, x.new_full((self.n_samples,), t, dtype=torch.long))\n",
    "\n",
    "            # Log the final denoised samples\n",
    "            tracker.save('sample', x)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train a Denoising Diffusion Probabilistic Model (DDPM) with the set dataloader.\n",
    "        \"\"\"\n",
    "        data_steps = 0\n",
    "        curr_loss = 0\n",
    "        # Iterate through the dataset\n",
    "        for data in monit.iterate('Train', self.data_loader):\n",
    "            # Increment global step\n",
    "            tracker.add_global_step()\n",
    "            # Move data to device\n",
    "            data = data.to(self.device)\n",
    "\n",
    "            # Make the gradients zero\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate loss\n",
    "            loss = self.diffusion.loss(data)\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            # Take an optimization step\n",
    "            self.optimizer.step()\n",
    "            # Track the loss\n",
    "            tracker.save('loss', loss)\n",
    "\n",
    "\n",
    "            # Diagnostics\n",
    "            curr_loss+=loss.item()\n",
    "            data_steps+=1\n",
    "\n",
    "            # print(\"EARLY STOP: DEBUG PURPOSES ONLY\")\n",
    "            # break\n",
    "\n",
    "        print(f\"Loss after {data_steps} input data seen: {round(curr_loss,2)}\")\n",
    "        with open('loss_log_'+MY_UUID+'_'+EXP+'.txt', 'a', ) as loss_log_file:\n",
    "            loss_info = \"{}, {}\".format(data_steps, curr_loss)\n",
    "            loss_log_file.write(loss_info+'\\n')\n",
    "\n",
    "\n",
    "        # print(data_steps, round(curr_loss, 2))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        ### Training loop\n",
    "        \"\"\"\n",
    "        for _ in monit.loop(self.epochs):\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            # Sample some images\n",
    "            # self.sample()\n",
    "            \n",
    "            # New line in the console\n",
    "            tracker.new_line()\n",
    "            # Save the model\n",
    "            experiment.save_checkpoint()\n",
    "\n",
    "            # break\n",
    "            print(f\"Epoch:\", _)\n",
    "\n",
    "\n",
    "class MNISTDataset(torchvision.datasets.MNIST):\n",
    "    \"\"\"\n",
    "    ### MNIST dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(image_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        super().__init__(str(lab.get_data_path()), train=True, download=True, transform=transform)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return super().__getitem__(item)[0]\n",
    "\n",
    "\n",
    "@option(Configs.dataset, 'MNIST')\n",
    "def mnist_dataset(c: Configs):\n",
    "    \"\"\"\n",
    "    Create MNIST dataset\n",
    "    \"\"\"\n",
    "    return MNISTDataset(c.image_size)\n",
    "\n",
    "\n",
    "class CIFAR10Dataset(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    ### MNIST dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(image_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        super().__init__(str(lab.get_data_path()), train=True, download=True, transform=transform)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return super().__getitem__(item)[0]\n",
    "\n",
    "\n",
    "@option(Configs.dataset, 'CIFAR10')\n",
    "def mnist_dataset(c: Configs):\n",
    "    \"\"\"\n",
    "    Create CIFAR10 dataset\n",
    "    \"\"\"\n",
    "    return CIFAR10Dataset(c.image_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidGitRepositoryError\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:247\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[1;34m(self, uuid, name, python_file, comment, writers, ignore_callers, tags, is_evaluate)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     repo \u001b[39m=\u001b[39m git\u001b[39m.\u001b[39;49mRepo(lab_singleton()\u001b[39m.\u001b[39;49mpath)\n\u001b[0;32m    249\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\git\\repo\\base.py:282\u001b[0m, in \u001b[0;36mRepo.__init__\u001b[1;34m(self, path, odbt, search_parent_directories, expand_vars)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39m# END working dir handling\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworking_dir: Optional[PathLike] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_working_tree_dir \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommon_dir\n\u001b[0;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGitCommandWrapperType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworking_dir)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\git\\repo\\base.py:363\u001b[0m, in \u001b[0;36mRepo.common_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m     \u001b[39m# or could return \"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m InvalidGitRepositoryError()\n",
      "\u001b[1;31mInvalidGitRepositoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m EXP \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresidual\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# 'recurrent', 'residual'\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# Create experiment\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m experiment\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     47\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdiffuse\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     48\u001b[0m     writers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mscreen\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabml\u001b[39;49m\u001b[39m'\u001b[39;49m},\n\u001b[0;32m     49\u001b[0m     uuid\u001b[39m=\u001b[39;49mMY_UUID,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[39m# Create configurations\u001b[39;00m\n\u001b[0;32m     53\u001b[0m configs \u001b[39m=\u001b[39m Configs()\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\experiment.py:92\u001b[0m, in \u001b[0;36mcreate\u001b[1;34m(uuid, name, python_file, comment, writers, ignore_callers, tags, disable_screen)\u001b[0m\n\u001b[0;32m     88\u001b[0m     uuid \u001b[39m=\u001b[39m generate_uuid()\n\u001b[0;32m     90\u001b[0m monitor()\u001b[39m.\u001b[39mclear()\n\u001b[1;32m---> 92\u001b[0m _create_experiment(uuid\u001b[39m=\u001b[39;49muuid,\n\u001b[0;32m     93\u001b[0m                    name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m     94\u001b[0m                    python_file\u001b[39m=\u001b[39;49mpython_file,\n\u001b[0;32m     95\u001b[0m                    comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m     96\u001b[0m                    writers\u001b[39m=\u001b[39;49mwriters,\n\u001b[0;32m     97\u001b[0m                    ignore_callers\u001b[39m=\u001b[39;49mignore_callers,\n\u001b[0;32m     98\u001b[0m                    tags\u001b[39m=\u001b[39;49mtags,\n\u001b[0;32m     99\u001b[0m                    is_evaluate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:561\u001b[0m, in \u001b[0;36mcreate_experiment\u001b[1;34m(uuid, name, python_file, comment, writers, ignore_callers, tags, is_evaluate)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_experiment\u001b[39m(\u001b[39m*\u001b[39m,\n\u001b[0;32m    551\u001b[0m                       uuid: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    552\u001b[0m                       name: Optional[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m                       tags: Optional[Set[\u001b[39mstr\u001b[39m]],\n\u001b[0;32m    558\u001b[0m                       is_evaluate: \u001b[39mbool\u001b[39m):\n\u001b[0;32m    559\u001b[0m     \u001b[39mglobal\u001b[39;00m _internal\n\u001b[1;32m--> 561\u001b[0m     _internal \u001b[39m=\u001b[39m Experiment(uuid\u001b[39m=\u001b[39;49muuid,\n\u001b[0;32m    562\u001b[0m                            name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    563\u001b[0m                            python_file\u001b[39m=\u001b[39;49mpython_file,\n\u001b[0;32m    564\u001b[0m                            comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m    565\u001b[0m                            writers\u001b[39m=\u001b[39;49mwriters,\n\u001b[0;32m    566\u001b[0m                            ignore_callers\u001b[39m=\u001b[39;49mignore_callers,\n\u001b[0;32m    567\u001b[0m                            tags\u001b[39m=\u001b[39;49mtags,\n\u001b[0;32m    568\u001b[0m                            is_evaluate\u001b[39m=\u001b[39;49mis_evaluate)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:259\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[1;34m(self, uuid, name, python_file, comment, writers, ignore_callers, tags, is_evaluate)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mexcept\u001b[39;00m (git\u001b[39m.\u001b[39mInvalidGitRepositoryError, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_colab() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_kaggle():\n\u001b[1;32m--> 259\u001b[0m         labml_notice([\u001b[39m\"\u001b[39;49m\u001b[39mNot a valid git repository: \u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    260\u001b[0m                       (\u001b[39mstr\u001b[39;49m(lab_singleton()\u001b[39m.\u001b[39;49mpath), Text\u001b[39m.\u001b[39;49mvalue)])\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mcommit \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mcommit_message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\utils\\notice.py:34\u001b[0m, in \u001b[0;36mlabml_notice\u001b[1;34m(message, is_danger, is_warn, is_lite)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_lite:\n\u001b[0;32m     32\u001b[0m     log\u001b[39m.\u001b[39mappend((\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m50\u001b[39m, Text\u001b[39m.\u001b[39msubtle))\n\u001b[1;32m---> 34\u001b[0m logger\u001b[39m.\u001b[39;49mlog(log)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\logger.py:135\u001b[0m, in \u001b[0;36mlog\u001b[1;34m(is_new_line, is_reset, *args)\u001b[0m\n\u001b[0;32m    133\u001b[0m     _internal()\u001b[39m.\u001b[39mlog([(message, \u001b[39mNone\u001b[39;00m)], is_new_line\u001b[39m=\u001b[39mis_new_line, is_reset\u001b[39m=\u001b[39mis_reset)\n\u001b[0;32m    134\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(message, \u001b[39mlist\u001b[39m):\n\u001b[1;32m--> 135\u001b[0m     _internal()\u001b[39m.\u001b[39mlog(message, is_new_line\u001b[39m=\u001b[39mis_new_line, is_reset\u001b[39m=\u001b[39mis_reset)\n\u001b[0;32m    136\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(message)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, message)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\logger\\__init__.py:36\u001b[0m, in \u001b[0;36mlogger_singleton\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mglobal\u001b[39;00m _internal\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m _internal \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     _internal \u001b[39m=\u001b[39m Logger()\n\u001b[0;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m _internal\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\logger\\__init__.py:17\u001b[0m, in \u001b[0;36mLogger.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__destinations \u001b[39m=\u001b[39m create_destination()\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__inspect \u001b[39m=\u001b[39m Inspect(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\logger\\destinations\\factory.py:12\u001b[0m, in \u001b[0;36mcreate_destination\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m is_ipynb():\n\u001b[0;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m is_ipynb_pycharm():\n\u001b[1;32m---> 12\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mlabml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdestinations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mipynb_pycharm\u001b[39;00m \u001b[39mimport\u001b[39;00m IpynbPyCharmDestination\n\u001b[0;32m     13\u001b[0m         \u001b[39mreturn\u001b[39;00m [IpynbPyCharmDestination(), ConsoleDestination(\u001b[39mFalse\u001b[39;00m)]\n\u001b[0;32m     14\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\ANACONDA\\envs\\pytorch-gpu\\lib\\site-packages\\labml\\internal\\logger\\destinations\\ipynb_pycharm.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Union, Tuple, Callable, Optional\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m display\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mipywidgets\u001b[39;00m \u001b[39mimport\u001b[39;00m HTML\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlabml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m StyleCode\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlabml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdestinations\u001b[39;00m \u001b[39mimport\u001b[39;00m Destination\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579b3ba4908743a3bb65d26b175b13bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Device is using GPU: False\n",
      "The total number of parameters are: 129079233\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"down.0.re.conv_input.weight\", \"down.0.re.norm_input.weight\", \"down.0.re.norm_input.bias\", \"down.0.re.skip.weight\", \"down.0.re.norm_skip.weight\", \"down.0.re.norm_skip.bias\", \"down.0.re.conv3.weight\", \"down.0.re.norm1_0.weight\", \"down.0.re.norm1_0.bias\", \"down.0.re.norm3_0.weight\", \"down.0.re.norm3_0.bias\", \"down.1.re.conv_input.weight\", \"down.1.re.norm_input.weight\", \"down.1.re.norm_input.bias\", \"down.1.re.skip.weight\", \"down.1.re.norm_skip.weight\", \"down.1.re.norm_skip.bias\", \"down.1.re.conv3.weight\", \"down.1.re.norm1_0.weight\", \"down.1.re.norm1_0.bias\", \"down.1.re.norm3_0.weight\", \"down.1.re.norm3_0.bias\", \"down.3.re.conv_input.weight\", \"down.3.re.norm_input.weight\", \"down.3.re.norm_input.bias\", \"down.3.re.skip.weight\", \"down.3.re.norm_skip.weight\", \"down.3.re.norm_skip.bias\", \"down.3.re.conv3.weight\", \"down.3.re.norm1_0.weight\", \"down.3.re.norm1_0.bias\", \"down.3.re.norm3_0.weight\", \"down.3.re.norm3_0.bias\", \"down.4.re.conv_input.weight\", \"down.4.re.norm_input.weight\", \"down.4.re.norm_input.bias\", \"down.4.re.skip.weight\", \"down.4.re.norm_skip.weight\", \"down.4.re.norm_skip.bias\", \"down.4.re.conv3.weight\", \"down.4.re.norm1_0.weight\", \"down.4.re.norm1_0.bias\", \"down.4.re.norm3_0.weight\", \"down.4.re.norm3_0.bias\", \"down.6.re.conv_input.weight\", \"down.6.re.norm_input.weight\", \"down.6.re.norm_input.bias\", \"down.6.re.skip.weight\", \"down.6.re.norm_skip.weight\", \"down.6.re.norm_skip.bias\", \"down.6.re.conv3.weight\", \"down.6.re.norm1_0.weight\", \"down.6.re.norm1_0.bias\", \"down.6.re.norm3_0.weight\", \"down.6.re.norm3_0.bias\", \"down.7.re.conv_input.weight\", \"down.7.re.norm_input.weight\", \"down.7.re.norm_input.bias\", \"down.7.re.skip.weight\", \"down.7.re.norm_skip.weight\", \"down.7.re.norm_skip.bias\", \"down.7.re.conv3.weight\", \"down.7.re.norm1_0.weight\", \"down.7.re.norm1_0.bias\", \"down.7.re.norm3_0.weight\", \"down.7.re.norm3_0.bias\", \"down.9.re.conv_input.weight\", \"down.9.re.norm_input.weight\", \"down.9.re.norm_input.bias\", \"down.9.re.skip.weight\", \"down.9.re.norm_skip.weight\", \"down.9.re.norm_skip.bias\", \"down.9.re.conv3.weight\", \"down.9.re.norm1_0.weight\", \"down.9.re.norm1_0.bias\", \"down.9.re.norm3_0.weight\", \"down.9.re.norm3_0.bias\", \"down.10.re.conv_input.weight\", \"down.10.re.norm_input.weight\", \"down.10.re.norm_input.bias\", \"down.10.re.skip.weight\", \"down.10.re.norm_skip.weight\", \"down.10.re.norm_skip.bias\", \"down.10.re.conv3.weight\", \"down.10.re.norm1_0.weight\", \"down.10.re.norm1_0.bias\", \"down.10.re.norm3_0.weight\", \"down.10.re.norm3_0.bias\", \"middle.re1.conv_input.weight\", \"middle.re1.norm_input.weight\", \"middle.re1.norm_input.bias\", \"middle.re1.skip.weight\", \"middle.re1.norm_skip.weight\", \"middle.re1.norm_skip.bias\", \"middle.re1.conv3.weight\", \"middle.re1.norm1_0.weight\", \"middle.re1.norm1_0.bias\", \"middle.re1.norm3_0.weight\", \"middle.re1.norm3_0.bias\", \"middle.re2.conv_input.weight\", \"middle.re2.norm_input.weight\", \"middle.re2.norm_input.bias\", \"middle.re2.skip.weight\", \"middle.re2.norm_skip.weight\", \"middle.re2.norm_skip.bias\", \"middle.re2.conv3.weight\", \"middle.re2.norm1_0.weight\", \"middle.re2.norm1_0.bias\", \"middle.re2.norm3_0.weight\", \"middle.re2.norm3_0.bias\", \"up.0.re.conv_input.weight\", \"up.0.re.norm_input.weight\", \"up.0.re.norm_input.bias\", \"up.0.re.skip.weight\", \"up.0.re.norm_skip.weight\", \"up.0.re.norm_skip.bias\", \"up.0.re.conv3.weight\", \"up.0.re.norm1_0.weight\", \"up.0.re.norm1_0.bias\", \"up.0.re.norm3_0.weight\", \"up.0.re.norm3_0.bias\", \"up.1.re.conv_input.weight\", \"up.1.re.norm_input.weight\", \"up.1.re.norm_input.bias\", \"up.1.re.skip.weight\", \"up.1.re.norm_skip.weight\", \"up.1.re.norm_skip.bias\", \"up.1.re.conv3.weight\", \"up.1.re.norm1_0.weight\", \"up.1.re.norm1_0.bias\", \"up.1.re.norm3_0.weight\", \"up.1.re.norm3_0.bias\", \"up.2.re.conv_input.weight\", \"up.2.re.norm_input.weight\", \"up.2.re.norm_input.bias\", \"up.2.re.skip.weight\", \"up.2.re.norm_skip.weight\", \"up.2.re.norm_skip.bias\", \"up.2.re.conv3.weight\", \"up.2.re.norm1_0.weight\", \"up.2.re.norm1_0.bias\", \"up.2.re.norm3_0.weight\", \"up.2.re.norm3_0.bias\", \"up.4.re.conv_input.weight\", \"up.4.re.norm_input.weight\", \"up.4.re.norm_input.bias\", \"up.4.re.skip.weight\", \"up.4.re.norm_skip.weight\", \"up.4.re.norm_skip.bias\", \"up.4.re.conv3.weight\", \"up.4.re.norm1_0.weight\", \"up.4.re.norm1_0.bias\", \"up.4.re.norm3_0.weight\", \"up.4.re.norm3_0.bias\", \"up.5.re.conv_input.weight\", \"up.5.re.norm_input.weight\", \"up.5.re.norm_input.bias\", \"up.5.re.skip.weight\", \"up.5.re.norm_skip.weight\", \"up.5.re.norm_skip.bias\", \"up.5.re.conv3.weight\", \"up.5.re.norm1_0.weight\", \"up.5.re.norm1_0.bias\", \"up.5.re.norm3_0.weight\", \"up.5.re.norm3_0.bias\", \"up.6.re.conv_input.weight\", \"up.6.re.norm_input.weight\", \"up.6.re.norm_input.bias\", \"up.6.re.skip.weight\", \"up.6.re.norm_skip.weight\", \"up.6.re.norm_skip.bias\", \"up.6.re.conv3.weight\", \"up.6.re.norm1_0.weight\", \"up.6.re.norm1_0.bias\", \"up.6.re.norm3_0.weight\", \"up.6.re.norm3_0.bias\", \"up.8.re.conv_input.weight\", \"up.8.re.norm_input.weight\", \"up.8.re.norm_input.bias\", \"up.8.re.skip.weight\", \"up.8.re.norm_skip.weight\", \"up.8.re.norm_skip.bias\", \"up.8.re.conv3.weight\", \"up.8.re.norm1_0.weight\", \"up.8.re.norm1_0.bias\", \"up.8.re.norm3_0.weight\", \"up.8.re.norm3_0.bias\", \"up.9.re.conv_input.weight\", \"up.9.re.norm_input.weight\", \"up.9.re.norm_input.bias\", \"up.9.re.skip.weight\", \"up.9.re.norm_skip.weight\", \"up.9.re.norm_skip.bias\", \"up.9.re.conv3.weight\", \"up.9.re.norm1_0.weight\", \"up.9.re.norm1_0.bias\", \"up.9.re.norm3_0.weight\", \"up.9.re.norm3_0.bias\", \"up.10.re.conv_input.weight\", \"up.10.re.norm_input.weight\", \"up.10.re.norm_input.bias\", \"up.10.re.skip.weight\", \"up.10.re.norm_skip.weight\", \"up.10.re.norm_skip.bias\", \"up.10.re.conv3.weight\", \"up.10.re.norm1_0.weight\", \"up.10.re.norm1_0.bias\", \"up.10.re.norm3_0.weight\", \"up.10.re.norm3_0.bias\", \"up.12.re.conv_input.weight\", \"up.12.re.norm_input.weight\", \"up.12.re.norm_input.bias\", \"up.12.re.skip.weight\", \"up.12.re.norm_skip.weight\", \"up.12.re.norm_skip.bias\", \"up.12.re.conv3.weight\", \"up.12.re.norm1_0.weight\", \"up.12.re.norm1_0.bias\", \"up.12.re.norm3_0.weight\", \"up.12.re.norm3_0.bias\", \"up.13.re.conv_input.weight\", \"up.13.re.norm_input.weight\", \"up.13.re.norm_input.bias\", \"up.13.re.skip.weight\", \"up.13.re.norm_skip.weight\", \"up.13.re.norm_skip.bias\", \"up.13.re.conv3.weight\", \"up.13.re.norm1_0.weight\", \"up.13.re.norm1_0.bias\", \"up.13.re.norm3_0.weight\", \"up.13.re.norm3_0.bias\", \"up.14.re.conv_input.weight\", \"up.14.re.norm_input.weight\", \"up.14.re.norm_input.bias\", \"up.14.re.skip.weight\", \"up.14.re.norm_skip.weight\", \"up.14.re.norm_skip.bias\", \"up.14.re.conv3.weight\", \"up.14.re.norm1_0.weight\", \"up.14.re.norm1_0.bias\", \"up.14.re.norm3_0.weight\", \"up.14.re.norm3_0.bias\". \n\tUnexpected key(s) in state_dict: \"down.0.re.norm1.weight\", \"down.0.re.norm1.bias\", \"down.0.re.norm2.weight\", \"down.0.re.norm2.bias\", \"down.0.re.conv2.weight\", \"down.0.re.conv2.bias\", \"down.0.re.conv1.bias\", \"down.1.re.norm1.weight\", \"down.1.re.norm1.bias\", \"down.1.re.norm2.weight\", \"down.1.re.norm2.bias\", \"down.1.re.conv2.weight\", \"down.1.re.conv2.bias\", \"down.1.re.conv1.bias\", \"down.3.re.norm1.weight\", \"down.3.re.norm1.bias\", \"down.3.re.norm2.weight\", \"down.3.re.norm2.bias\", \"down.3.re.conv2.weight\", \"down.3.re.conv2.bias\", \"down.3.re.conv1.bias\", \"down.4.re.norm1.weight\", \"down.4.re.norm1.bias\", \"down.4.re.norm2.weight\", \"down.4.re.norm2.bias\", \"down.4.re.conv2.weight\", \"down.4.re.conv2.bias\", \"down.4.re.conv1.bias\", \"down.6.re.norm1.weight\", \"down.6.re.norm1.bias\", \"down.6.re.norm2.weight\", \"down.6.re.norm2.bias\", \"down.6.re.conv2.weight\", \"down.6.re.conv2.bias\", \"down.6.re.conv1.bias\", \"down.7.re.norm1.weight\", \"down.7.re.norm1.bias\", \"down.7.re.norm2.weight\", \"down.7.re.norm2.bias\", \"down.7.re.conv2.weight\", \"down.7.re.conv2.bias\", \"down.7.re.conv1.bias\", \"down.9.re.norm1.weight\", \"down.9.re.norm1.bias\", \"down.9.re.norm2.weight\", \"down.9.re.norm2.bias\", \"down.9.re.conv2.weight\", \"down.9.re.conv2.bias\", \"down.9.re.conv1.bias\", \"down.10.re.norm1.weight\", \"down.10.re.norm1.bias\", \"down.10.re.norm2.weight\", \"down.10.re.norm2.bias\", \"down.10.re.conv2.weight\", \"down.10.re.conv2.bias\", \"down.10.re.conv1.bias\", \"middle.re1.norm1.weight\", \"middle.re1.norm1.bias\", \"middle.re1.norm2.weight\", \"middle.re1.norm2.bias\", \"middle.re1.conv2.weight\", \"middle.re1.conv2.bias\", \"middle.re1.conv1.bias\", \"middle.re2.norm1.weight\", \"middle.re2.norm1.bias\", \"middle.re2.norm2.weight\", \"middle.re2.norm2.bias\", \"middle.re2.conv2.weight\", \"middle.re2.conv2.bias\", \"middle.re2.conv1.bias\", \"up.0.re.norm1.weight\", \"up.0.re.norm1.bias\", \"up.0.re.norm2.weight\", \"up.0.re.norm2.bias\", \"up.0.re.conv2.weight\", \"up.0.re.conv2.bias\", \"up.0.re.conv1.bias\", \"up.1.re.norm1.weight\", \"up.1.re.norm1.bias\", \"up.1.re.norm2.weight\", \"up.1.re.norm2.bias\", \"up.1.re.conv2.weight\", \"up.1.re.conv2.bias\", \"up.1.re.conv1.bias\", \"up.2.re.norm1.weight\", \"up.2.re.norm1.bias\", \"up.2.re.norm2.weight\", \"up.2.re.norm2.bias\", \"up.2.re.conv2.weight\", \"up.2.re.conv2.bias\", \"up.2.re.conv1.bias\", \"up.4.re.norm1.weight\", \"up.4.re.norm1.bias\", \"up.4.re.norm2.weight\", \"up.4.re.norm2.bias\", \"up.4.re.conv2.weight\", \"up.4.re.conv2.bias\", \"up.4.re.conv1.bias\", \"up.5.re.norm1.weight\", \"up.5.re.norm1.bias\", \"up.5.re.norm2.weight\", \"up.5.re.norm2.bias\", \"up.5.re.conv2.weight\", \"up.5.re.conv2.bias\", \"up.5.re.conv1.bias\", \"up.6.re.norm1.weight\", \"up.6.re.norm1.bias\", \"up.6.re.norm2.weight\", \"up.6.re.norm2.bias\", \"up.6.re.conv2.weight\", \"up.6.re.conv2.bias\", \"up.6.re.conv1.bias\", \"up.8.re.norm1.weight\", \"up.8.re.norm1.bias\", \"up.8.re.norm2.weight\", \"up.8.re.norm2.bias\", \"up.8.re.conv2.weight\", \"up.8.re.conv2.bias\", \"up.8.re.conv1.bias\", \"up.9.re.norm1.weight\", \"up.9.re.norm1.bias\", \"up.9.re.norm2.weight\", \"up.9.re.norm2.bias\", \"up.9.re.conv2.weight\", \"up.9.re.conv2.bias\", \"up.9.re.conv1.bias\", \"up.10.re.norm1.weight\", \"up.10.re.norm1.bias\", \"up.10.re.norm2.weight\", \"up.10.re.norm2.bias\", \"up.10.re.conv2.weight\", \"up.10.re.conv2.bias\", \"up.10.re.conv1.bias\", \"up.12.re.norm1.weight\", \"up.12.re.norm1.bias\", \"up.12.re.norm2.weight\", \"up.12.re.norm2.bias\", \"up.12.re.conv2.weight\", \"up.12.re.conv2.bias\", \"up.12.re.conv1.bias\", \"up.13.re.norm1.weight\", \"up.13.re.norm1.bias\", \"up.13.re.norm2.weight\", \"up.13.re.norm2.bias\", \"up.13.re.conv2.weight\", \"up.13.re.conv2.bias\", \"up.13.re.conv1.bias\", \"up.14.re.norm1.weight\", \"up.14.re.norm1.bias\", \"up.14.re.norm2.weight\", \"up.14.re.norm2.bias\", \"up.14.re.conv2.weight\", \"up.14.re.conv2.bias\", \"up.14.re.conv1.bias\". \n\tsize mismatch for down.0.re.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for down.0.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for down.0.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down.1.re.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for down.1.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for down.1.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down.3.re.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for down.3.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for down.3.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for down.4.re.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for down.4.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for down.4.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for down.6.re.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for down.6.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for down.6.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for down.7.re.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for down.7.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for down.7.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for down.9.re.conv1.weight: copying a param with shape torch.Size([1024, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for down.9.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for down.9.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for down.10.re.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for down.10.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for down.10.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for middle.re1.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for middle.re1.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for middle.re1.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for middle.re2.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for middle.re2.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for middle.re2.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.0.re.conv1.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for up.0.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for up.0.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.1.re.conv1.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for up.1.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for up.1.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.2.re.conv1.weight: copying a param with shape torch.Size([256, 1280, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.2.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.2.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.4.re.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.4.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.4.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.5.re.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.5.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.5.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.6.re.conv1.weight: copying a param with shape torch.Size([128, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.6.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.6.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.8.re.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.8.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.8.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.9.re.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.9.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.9.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.10.re.conv1.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.10.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.10.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.12.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.12.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.12.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.13.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.13.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.13.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.14.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.14.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.14.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[15], line 99\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m experiment\u001b[39m.\u001b[39mload(run_uuid\u001b[39m=\u001b[39mcheckpoint_uuid) \u001b[39m# Note: passing 'run_uuid=UUID' will try restoring from a checkpoint within current run (or so I think)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# Start the experiment (note: not using with experiment.start() when loading)\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[39mwith\u001b[39;00m experiment\u001b[39m.\u001b[39;49mstart():\n\u001b[0;32m    100\u001b[0m     configs\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\experiment.py:278\u001b[0m, in \u001b[0;36mstart\u001b[1;34m()\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mglobal\u001b[39;00m _load_run_uuid\n\u001b[0;32m    276\u001b[0m \u001b[39mglobal\u001b[39;00m _load_checkpoint\n\u001b[1;32m--> 278\u001b[0m \u001b[39mreturn\u001b[39;00m _experiment_singleton()\u001b[39m.\u001b[39;49mstart(run_uuid\u001b[39m=\u001b[39;49m_load_run_uuid, checkpoint\u001b[39m=\u001b[39;49m_load_checkpoint)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:442\u001b[0m, in \u001b[0;36mExperiment.start\u001b[1;34m(self, run_uuid, checkpoint)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m checkpoint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m         checkpoint \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 442\u001b[0m     global_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__start_from_checkpoint(run_uuid, checkpoint)\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    444\u001b[0m     global_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:340\u001b[0m, in \u001b[0;36mExperiment.__start_from_checkpoint\u001b[1;34m(self, run_uuid, checkpoint)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mwith\u001b[39;00m monit\u001b[39m.\u001b[39msection(\u001b[39m\"\u001b[39m\u001b[39mLoading checkpoint\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 340\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_checkpoint(checkpoint_path)\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun\u001b[39m.\u001b[39mload_run \u001b[39m=\u001b[39m run_uuid\n\u001b[0;32m    343\u001b[0m \u001b[39mreturn\u001b[39;00m global_step\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:308\u001b[0m, in \u001b[0;36mExperiment._load_checkpoint\u001b[1;34m(self, checkpoint_path)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, checkpoint_path: pathlib\u001b[39m.\u001b[39mPath):\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_saver\u001b[39m.\u001b[39;49mload(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\internal\\experiment\\__init__.py:141\u001b[0m, in \u001b[0;36mCheckpointSaver.load\u001b[1;34m(self, checkpoint_path, models)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m to_load:\n\u001b[0;32m    140\u001b[0m     saver \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_savers[name]\n\u001b[1;32m--> 141\u001b[0m     saver\u001b[39m.\u001b[39;49mload(checkpoint_path, info[name])\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[0;32m    144\u001b[0m     labml_notice([(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, Text\u001b[39m.\u001b[39mhighlight),\n\u001b[0;32m    145\u001b[0m                   (\u001b[39m'\u001b[39m\u001b[39mmodel(s) could not be found.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m),\n\u001b[0;32m    146\u001b[0m                   (\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mto_load\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, Text\u001b[39m.\u001b[39mnone),\n\u001b[0;32m    147\u001b[0m                   (\u001b[39m'\u001b[39m\u001b[39mmodels were loaded.\u001b[39m\u001b[39m'\u001b[39m, Text\u001b[39m.\u001b[39mnone)\n\u001b[0;32m    148\u001b[0m                   ], is_danger\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\labml\\internal\\experiment\\pytorch.py:67\u001b[0m, in \u001b[0;36mPyTorchModelSaver.load\u001b[1;34m(self, checkpoint_path, info)\u001b[0m\n\u001b[0;32m     63\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mstr\u001b[39m(checkpoint_path \u001b[39m/\u001b[39m file_name), map_location\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(state)\n",
      "File \u001b[1;32mc:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UNet:\n\tMissing key(s) in state_dict: \"down.0.re.conv_input.weight\", \"down.0.re.norm_input.weight\", \"down.0.re.norm_input.bias\", \"down.0.re.skip.weight\", \"down.0.re.norm_skip.weight\", \"down.0.re.norm_skip.bias\", \"down.0.re.conv3.weight\", \"down.0.re.norm1_0.weight\", \"down.0.re.norm1_0.bias\", \"down.0.re.norm3_0.weight\", \"down.0.re.norm3_0.bias\", \"down.1.re.conv_input.weight\", \"down.1.re.norm_input.weight\", \"down.1.re.norm_input.bias\", \"down.1.re.skip.weight\", \"down.1.re.norm_skip.weight\", \"down.1.re.norm_skip.bias\", \"down.1.re.conv3.weight\", \"down.1.re.norm1_0.weight\", \"down.1.re.norm1_0.bias\", \"down.1.re.norm3_0.weight\", \"down.1.re.norm3_0.bias\", \"down.3.re.conv_input.weight\", \"down.3.re.norm_input.weight\", \"down.3.re.norm_input.bias\", \"down.3.re.skip.weight\", \"down.3.re.norm_skip.weight\", \"down.3.re.norm_skip.bias\", \"down.3.re.conv3.weight\", \"down.3.re.norm1_0.weight\", \"down.3.re.norm1_0.bias\", \"down.3.re.norm3_0.weight\", \"down.3.re.norm3_0.bias\", \"down.4.re.conv_input.weight\", \"down.4.re.norm_input.weight\", \"down.4.re.norm_input.bias\", \"down.4.re.skip.weight\", \"down.4.re.norm_skip.weight\", \"down.4.re.norm_skip.bias\", \"down.4.re.conv3.weight\", \"down.4.re.norm1_0.weight\", \"down.4.re.norm1_0.bias\", \"down.4.re.norm3_0.weight\", \"down.4.re.norm3_0.bias\", \"down.6.re.conv_input.weight\", \"down.6.re.norm_input.weight\", \"down.6.re.norm_input.bias\", \"down.6.re.skip.weight\", \"down.6.re.norm_skip.weight\", \"down.6.re.norm_skip.bias\", \"down.6.re.conv3.weight\", \"down.6.re.norm1_0.weight\", \"down.6.re.norm1_0.bias\", \"down.6.re.norm3_0.weight\", \"down.6.re.norm3_0.bias\", \"down.7.re.conv_input.weight\", \"down.7.re.norm_input.weight\", \"down.7.re.norm_input.bias\", \"down.7.re.skip.weight\", \"down.7.re.norm_skip.weight\", \"down.7.re.norm_skip.bias\", \"down.7.re.conv3.weight\", \"down.7.re.norm1_0.weight\", \"down.7.re.norm1_0.bias\", \"down.7.re.norm3_0.weight\", \"down.7.re.norm3_0.bias\", \"down.9.re.conv_input.weight\", \"down.9.re.norm_input.weight\", \"down.9.re.norm_input.bias\", \"down.9.re.skip.weight\", \"down.9.re.norm_skip.weight\", \"down.9.re.norm_skip.bias\", \"down.9.re.conv3.weight\", \"down.9.re.norm1_0.weight\", \"down.9.re.norm1_0.bias\", \"down.9.re.norm3_0.weight\", \"down.9.re.norm3_0.bias\", \"down.10.re.conv_input.weight\", \"down.10.re.norm_input.weight\", \"down.10.re.norm_input.bias\", \"down.10.re.skip.weight\", \"down.10.re.norm_skip.weight\", \"down.10.re.norm_skip.bias\", \"down.10.re.conv3.weight\", \"down.10.re.norm1_0.weight\", \"down.10.re.norm1_0.bias\", \"down.10.re.norm3_0.weight\", \"down.10.re.norm3_0.bias\", \"middle.re1.conv_input.weight\", \"middle.re1.norm_input.weight\", \"middle.re1.norm_input.bias\", \"middle.re1.skip.weight\", \"middle.re1.norm_skip.weight\", \"middle.re1.norm_skip.bias\", \"middle.re1.conv3.weight\", \"middle.re1.norm1_0.weight\", \"middle.re1.norm1_0.bias\", \"middle.re1.norm3_0.weight\", \"middle.re1.norm3_0.bias\", \"middle.re2.conv_input.weight\", \"middle.re2.norm_input.weight\", \"middle.re2.norm_input.bias\", \"middle.re2.skip.weight\", \"middle.re2.norm_skip.weight\", \"middle.re2.norm_skip.bias\", \"middle.re2.conv3.weight\", \"middle.re2.norm1_0.weight\", \"middle.re2.norm1_0.bias\", \"middle.re2.norm3_0.weight\", \"middle.re2.norm3_0.bias\", \"up.0.re.conv_input.weight\", \"up.0.re.norm_input.weight\", \"up.0.re.norm_input.bias\", \"up.0.re.skip.weight\", \"up.0.re.norm_skip.weight\", \"up.0.re.norm_skip.bias\", \"up.0.re.conv3.weight\", \"up.0.re.norm1_0.weight\", \"up.0.re.norm1_0.bias\", \"up.0.re.norm3_0.weight\", \"up.0.re.norm3_0.bias\", \"up.1.re.conv_input.weight\", \"up.1.re.norm_input.weight\", \"up.1.re.norm_input.bias\", \"up.1.re.skip.weight\", \"up.1.re.norm_skip.weight\", \"up.1.re.norm_skip.bias\", \"up.1.re.conv3.weight\", \"up.1.re.norm1_0.weight\", \"up.1.re.norm1_0.bias\", \"up.1.re.norm3_0.weight\", \"up.1.re.norm3_0.bias\", \"up.2.re.conv_input.weight\", \"up.2.re.norm_input.weight\", \"up.2.re.norm_input.bias\", \"up.2.re.skip.weight\", \"up.2.re.norm_skip.weight\", \"up.2.re.norm_skip.bias\", \"up.2.re.conv3.weight\", \"up.2.re.norm1_0.weight\", \"up.2.re.norm1_0.bias\", \"up.2.re.norm3_0.weight\", \"up.2.re.norm3_0.bias\", \"up.4.re.conv_input.weight\", \"up.4.re.norm_input.weight\", \"up.4.re.norm_input.bias\", \"up.4.re.skip.weight\", \"up.4.re.norm_skip.weight\", \"up.4.re.norm_skip.bias\", \"up.4.re.conv3.weight\", \"up.4.re.norm1_0.weight\", \"up.4.re.norm1_0.bias\", \"up.4.re.norm3_0.weight\", \"up.4.re.norm3_0.bias\", \"up.5.re.conv_input.weight\", \"up.5.re.norm_input.weight\", \"up.5.re.norm_input.bias\", \"up.5.re.skip.weight\", \"up.5.re.norm_skip.weight\", \"up.5.re.norm_skip.bias\", \"up.5.re.conv3.weight\", \"up.5.re.norm1_0.weight\", \"up.5.re.norm1_0.bias\", \"up.5.re.norm3_0.weight\", \"up.5.re.norm3_0.bias\", \"up.6.re.conv_input.weight\", \"up.6.re.norm_input.weight\", \"up.6.re.norm_input.bias\", \"up.6.re.skip.weight\", \"up.6.re.norm_skip.weight\", \"up.6.re.norm_skip.bias\", \"up.6.re.conv3.weight\", \"up.6.re.norm1_0.weight\", \"up.6.re.norm1_0.bias\", \"up.6.re.norm3_0.weight\", \"up.6.re.norm3_0.bias\", \"up.8.re.conv_input.weight\", \"up.8.re.norm_input.weight\", \"up.8.re.norm_input.bias\", \"up.8.re.skip.weight\", \"up.8.re.norm_skip.weight\", \"up.8.re.norm_skip.bias\", \"up.8.re.conv3.weight\", \"up.8.re.norm1_0.weight\", \"up.8.re.norm1_0.bias\", \"up.8.re.norm3_0.weight\", \"up.8.re.norm3_0.bias\", \"up.9.re.conv_input.weight\", \"up.9.re.norm_input.weight\", \"up.9.re.norm_input.bias\", \"up.9.re.skip.weight\", \"up.9.re.norm_skip.weight\", \"up.9.re.norm_skip.bias\", \"up.9.re.conv3.weight\", \"up.9.re.norm1_0.weight\", \"up.9.re.norm1_0.bias\", \"up.9.re.norm3_0.weight\", \"up.9.re.norm3_0.bias\", \"up.10.re.conv_input.weight\", \"up.10.re.norm_input.weight\", \"up.10.re.norm_input.bias\", \"up.10.re.skip.weight\", \"up.10.re.norm_skip.weight\", \"up.10.re.norm_skip.bias\", \"up.10.re.conv3.weight\", \"up.10.re.norm1_0.weight\", \"up.10.re.norm1_0.bias\", \"up.10.re.norm3_0.weight\", \"up.10.re.norm3_0.bias\", \"up.12.re.conv_input.weight\", \"up.12.re.norm_input.weight\", \"up.12.re.norm_input.bias\", \"up.12.re.skip.weight\", \"up.12.re.norm_skip.weight\", \"up.12.re.norm_skip.bias\", \"up.12.re.conv3.weight\", \"up.12.re.norm1_0.weight\", \"up.12.re.norm1_0.bias\", \"up.12.re.norm3_0.weight\", \"up.12.re.norm3_0.bias\", \"up.13.re.conv_input.weight\", \"up.13.re.norm_input.weight\", \"up.13.re.norm_input.bias\", \"up.13.re.skip.weight\", \"up.13.re.norm_skip.weight\", \"up.13.re.norm_skip.bias\", \"up.13.re.conv3.weight\", \"up.13.re.norm1_0.weight\", \"up.13.re.norm1_0.bias\", \"up.13.re.norm3_0.weight\", \"up.13.re.norm3_0.bias\", \"up.14.re.conv_input.weight\", \"up.14.re.norm_input.weight\", \"up.14.re.norm_input.bias\", \"up.14.re.skip.weight\", \"up.14.re.norm_skip.weight\", \"up.14.re.norm_skip.bias\", \"up.14.re.conv3.weight\", \"up.14.re.norm1_0.weight\", \"up.14.re.norm1_0.bias\", \"up.14.re.norm3_0.weight\", \"up.14.re.norm3_0.bias\". \n\tUnexpected key(s) in state_dict: \"down.0.re.norm1.weight\", \"down.0.re.norm1.bias\", \"down.0.re.norm2.weight\", \"down.0.re.norm2.bias\", \"down.0.re.conv2.weight\", \"down.0.re.conv2.bias\", \"down.0.re.conv1.bias\", \"down.1.re.norm1.weight\", \"down.1.re.norm1.bias\", \"down.1.re.norm2.weight\", \"down.1.re.norm2.bias\", \"down.1.re.conv2.weight\", \"down.1.re.conv2.bias\", \"down.1.re.conv1.bias\", \"down.3.re.norm1.weight\", \"down.3.re.norm1.bias\", \"down.3.re.norm2.weight\", \"down.3.re.norm2.bias\", \"down.3.re.conv2.weight\", \"down.3.re.conv2.bias\", \"down.3.re.conv1.bias\", \"down.4.re.norm1.weight\", \"down.4.re.norm1.bias\", \"down.4.re.norm2.weight\", \"down.4.re.norm2.bias\", \"down.4.re.conv2.weight\", \"down.4.re.conv2.bias\", \"down.4.re.conv1.bias\", \"down.6.re.norm1.weight\", \"down.6.re.norm1.bias\", \"down.6.re.norm2.weight\", \"down.6.re.norm2.bias\", \"down.6.re.conv2.weight\", \"down.6.re.conv2.bias\", \"down.6.re.conv1.bias\", \"down.7.re.norm1.weight\", \"down.7.re.norm1.bias\", \"down.7.re.norm2.weight\", \"down.7.re.norm2.bias\", \"down.7.re.conv2.weight\", \"down.7.re.conv2.bias\", \"down.7.re.conv1.bias\", \"down.9.re.norm1.weight\", \"down.9.re.norm1.bias\", \"down.9.re.norm2.weight\", \"down.9.re.norm2.bias\", \"down.9.re.conv2.weight\", \"down.9.re.conv2.bias\", \"down.9.re.conv1.bias\", \"down.10.re.norm1.weight\", \"down.10.re.norm1.bias\", \"down.10.re.norm2.weight\", \"down.10.re.norm2.bias\", \"down.10.re.conv2.weight\", \"down.10.re.conv2.bias\", \"down.10.re.conv1.bias\", \"middle.re1.norm1.weight\", \"middle.re1.norm1.bias\", \"middle.re1.norm2.weight\", \"middle.re1.norm2.bias\", \"middle.re1.conv2.weight\", \"middle.re1.conv2.bias\", \"middle.re1.conv1.bias\", \"middle.re2.norm1.weight\", \"middle.re2.norm1.bias\", \"middle.re2.norm2.weight\", \"middle.re2.norm2.bias\", \"middle.re2.conv2.weight\", \"middle.re2.conv2.bias\", \"middle.re2.conv1.bias\", \"up.0.re.norm1.weight\", \"up.0.re.norm1.bias\", \"up.0.re.norm2.weight\", \"up.0.re.norm2.bias\", \"up.0.re.conv2.weight\", \"up.0.re.conv2.bias\", \"up.0.re.conv1.bias\", \"up.1.re.norm1.weight\", \"up.1.re.norm1.bias\", \"up.1.re.norm2.weight\", \"up.1.re.norm2.bias\", \"up.1.re.conv2.weight\", \"up.1.re.conv2.bias\", \"up.1.re.conv1.bias\", \"up.2.re.norm1.weight\", \"up.2.re.norm1.bias\", \"up.2.re.norm2.weight\", \"up.2.re.norm2.bias\", \"up.2.re.conv2.weight\", \"up.2.re.conv2.bias\", \"up.2.re.conv1.bias\", \"up.4.re.norm1.weight\", \"up.4.re.norm1.bias\", \"up.4.re.norm2.weight\", \"up.4.re.norm2.bias\", \"up.4.re.conv2.weight\", \"up.4.re.conv2.bias\", \"up.4.re.conv1.bias\", \"up.5.re.norm1.weight\", \"up.5.re.norm1.bias\", \"up.5.re.norm2.weight\", \"up.5.re.norm2.bias\", \"up.5.re.conv2.weight\", \"up.5.re.conv2.bias\", \"up.5.re.conv1.bias\", \"up.6.re.norm1.weight\", \"up.6.re.norm1.bias\", \"up.6.re.norm2.weight\", \"up.6.re.norm2.bias\", \"up.6.re.conv2.weight\", \"up.6.re.conv2.bias\", \"up.6.re.conv1.bias\", \"up.8.re.norm1.weight\", \"up.8.re.norm1.bias\", \"up.8.re.norm2.weight\", \"up.8.re.norm2.bias\", \"up.8.re.conv2.weight\", \"up.8.re.conv2.bias\", \"up.8.re.conv1.bias\", \"up.9.re.norm1.weight\", \"up.9.re.norm1.bias\", \"up.9.re.norm2.weight\", \"up.9.re.norm2.bias\", \"up.9.re.conv2.weight\", \"up.9.re.conv2.bias\", \"up.9.re.conv1.bias\", \"up.10.re.norm1.weight\", \"up.10.re.norm1.bias\", \"up.10.re.norm2.weight\", \"up.10.re.norm2.bias\", \"up.10.re.conv2.weight\", \"up.10.re.conv2.bias\", \"up.10.re.conv1.bias\", \"up.12.re.norm1.weight\", \"up.12.re.norm1.bias\", \"up.12.re.norm2.weight\", \"up.12.re.norm2.bias\", \"up.12.re.conv2.weight\", \"up.12.re.conv2.bias\", \"up.12.re.conv1.bias\", \"up.13.re.norm1.weight\", \"up.13.re.norm1.bias\", \"up.13.re.norm2.weight\", \"up.13.re.norm2.bias\", \"up.13.re.conv2.weight\", \"up.13.re.conv2.bias\", \"up.13.re.conv1.bias\", \"up.14.re.norm1.weight\", \"up.14.re.norm1.bias\", \"up.14.re.norm2.weight\", \"up.14.re.norm2.bias\", \"up.14.re.conv2.weight\", \"up.14.re.conv2.bias\", \"up.14.re.conv1.bias\". \n\tsize mismatch for down.0.re.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for down.0.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for down.0.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down.1.re.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for down.1.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for down.1.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for down.3.re.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for down.3.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for down.3.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for down.4.re.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for down.4.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for down.4.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for down.6.re.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for down.6.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for down.6.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for down.7.re.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for down.7.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for down.7.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for down.9.re.conv1.weight: copying a param with shape torch.Size([1024, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for down.9.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for down.9.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for down.10.re.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for down.10.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for down.10.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for middle.re1.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for middle.re1.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for middle.re1.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for middle.re2.conv1.weight: copying a param with shape torch.Size([1024, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for middle.re2.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for middle.re2.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.0.re.conv1.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for up.0.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for up.0.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.1.re.conv1.weight: copying a param with shape torch.Size([1024, 2048, 3, 3]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for up.1.re.time_emb.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for up.1.re.time_emb.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for up.2.re.conv1.weight: copying a param with shape torch.Size([256, 1280, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.2.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.2.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.4.re.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.4.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.4.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.5.re.conv1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for up.5.re.time_emb.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for up.5.re.time_emb.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for up.6.re.conv1.weight: copying a param with shape torch.Size([128, 384, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.6.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.6.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.8.re.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.8.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.8.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.9.re.conv1.weight: copying a param with shape torch.Size([128, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for up.9.re.time_emb.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for up.9.re.time_emb.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for up.10.re.conv1.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.10.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.10.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.12.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.12.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.12.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.13.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.13.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.13.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for up.14.re.conv1.weight: copying a param with shape torch.Size([64, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for up.14.re.time_emb.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for up.14.re.time_emb.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128])."
     ]
    }
   ],
   "source": [
    "# REMEMBER: set LOAD_CHECKPOINT to True if you want to load.\n",
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa9ee6e5d384a07ace6351cf500d993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1B…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters are: 173987521\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Denoising Diffusion Probabilistic Models (DDPM) evaluation/sampling\n",
    "summary: >\n",
    "  Code to generate samples from a trained\n",
    "  Denoising Diffusion Probabilistic Model.\n",
    "---\n",
    "# [Denoising Diffusion Probabilistic Models (DDPM)](index.html) evaluation/sampling\n",
    "This is the code to generate images and create interpolations between given images.\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image, resize\n",
    "\n",
    "from labml import experiment, monit\n",
    "from noise import DenoiseDiffusion, gather\n",
    "# from __main__ import Configs\n",
    "\n",
    "from labml import lab, tracker, experiment, monit\n",
    "from labml.configs import BaseConfigs, option\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "from noise import DenoiseDiffusion\n",
    "from unet import UNet\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Sampler:\n",
    "    \"\"\"\n",
    "    ## Sampler class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diffusion: DenoiseDiffusion, image_channels: int, image_size: int, device: torch.device):\n",
    "        \"\"\"\n",
    "        * `diffusion` is the `DenoiseDiffusion` instance\n",
    "        * `image_channels` is the number of channels in the image\n",
    "        * `image_size` is the image size\n",
    "        * `device` is the device of the model\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.image_size = image_size\n",
    "        self.image_channels = image_channels\n",
    "        self.diffusion = diffusion\n",
    "\n",
    "        # $T$\n",
    "        self.n_steps = diffusion.n_steps\n",
    "        # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n",
    "        self.eps_model = diffusion.eps_model\n",
    "        # $\\beta_t$\n",
    "        self.beta = diffusion.beta\n",
    "        # $\\alpha_t$\n",
    "        self.alpha = diffusion.alpha\n",
    "        # $\\bar\\alpha_t$\n",
    "        self.alpha_bar = diffusion.alpha_bar\n",
    "        # $\\bar\\alpha_{t-1}$\n",
    "        alpha_bar_tm1 = torch.cat([self.alpha_bar.new_ones((1,)), self.alpha_bar[:-1]])\n",
    "\n",
    "        # To calculate\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # q(x_{t-1}|x_t, x_0) &= \\mathcal{N} \\Big(x_{t-1}; \\tilde\\mu_t(x_t, x_0), \\tilde\\beta_t \\mathbf{I} \\Big) \\\\\n",
    "        # \\tilde\\mu_t(x_t, x_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n",
    "        #                          + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n",
    "        # \\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{a}\n",
    "        # \\end{align}\n",
    "\n",
    "        # $\\tilde\\beta_t$\n",
    "        self.beta_tilde = self.beta * (1 - alpha_bar_tm1) / (1 - self.alpha_bar)\n",
    "        # $$\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$$\n",
    "        self.mu_tilde_coef1 = self.beta * (alpha_bar_tm1 ** 0.5) / (1 - self.alpha_bar)\n",
    "        # $$\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}$$\n",
    "        self.mu_tilde_coef2 = (self.alpha ** 0.5) * (1 - alpha_bar_tm1) / (1 - self.alpha_bar)\n",
    "        # $\\sigma^2 = \\beta$\n",
    "        self.sigma2 = self.beta\n",
    "\n",
    "    def show_image(self, img, title=\"\"):\n",
    "        \"\"\"Helper function to display an image\"\"\"\n",
    "        img = img.clip(0, 1)\n",
    "        img = img.cpu().numpy()\n",
    "        plt.imshow(img.transpose(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        dirs = 'result_image'\n",
    "        if not os.path.exists(dirs):\n",
    "            os.makedirs(dirs)\n",
    "        path = dirs + '/' + title + '.jpg'\n",
    "        plt.savefig(path)\n",
    "        # plt.title(title)\n",
    "        # plt.show()\n",
    "\n",
    "    def make_video(self, frames, path=\"video.mp4\"):\n",
    "        \"\"\"Helper function to create a video\"\"\"\n",
    "        import imageio\n",
    "        # 20 second video\n",
    "        writer = imageio.get_writer(path, fps=len(frames) // 20)\n",
    "        # Add each image\n",
    "        for f in frames:\n",
    "            f = f.clip(0, 1)\n",
    "            f = to_pil_image(resize(f, [368, 368]))\n",
    "            writer.append_data(np.array(f))\n",
    "        #\n",
    "        writer.close()\n",
    "\n",
    "    def sample_animation(self, n_frames: int = 1000, create_video: bool = True):\n",
    "        \"\"\"\n",
    "        #### Sample an image step-by-step using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "        We sample an image step-by-step using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$ and at each step\n",
    "        show the estimate\n",
    "        $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n",
    "         \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n",
    "        \"\"\"\n",
    "\n",
    "        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n",
    "        xt = torch.randn([1, self.image_channels, self.image_size, self.image_size], device=self.device)\n",
    "\n",
    "        # Interval to log $\\hat{x}_0$\n",
    "        interval = self.n_steps // n_frames\n",
    "        # Frames for video\n",
    "        frames = []\n",
    "        # Sample $T$ steps\n",
    "        for t_inv in monit.iterate('Denoise', self.n_steps):\n",
    "            # $t$\n",
    "            t_ = self.n_steps - t_inv - 1\n",
    "            # $t$ in a tensor\n",
    "            t = xt.new_full((1,), t_, dtype=torch.long)\n",
    "            # $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n",
    "            eps_theta = self.eps_model(xt, t)\n",
    "            if t_ % interval == 0:\n",
    "                # Get $\\hat{x}_0$ and add to frames\n",
    "                x0 = self.p_x0(xt, t, eps_theta)\n",
    "                frames.append(x0[0])\n",
    "                if not create_video:\n",
    "                    self.show_image(x0[0], f\"{t_}\")\n",
    "            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "            xt = self.p_sample(xt, t, eps_theta)\n",
    "\n",
    "        # Make video\n",
    "        if create_video:\n",
    "            self.make_video(frames)\n",
    "\n",
    "    def interpolate(self, x1: torch.Tensor, x2: torch.Tensor, lambda_: float, t_: int = 100):\n",
    "        \"\"\"\n",
    "        #### Interpolate two images $x_0$ and $x'_0$\n",
    "        We get $x_t \\sim q(x_t|x_0)$ and $x'_t \\sim q(x'_t|x_0)$.\n",
    "        Then interpolate to\n",
    "         $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n",
    "        Then get\n",
    "         $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n",
    "        * `x1` is $x_0$\n",
    "        * `x2` is $x'_0$\n",
    "        * `lambda_` is $\\lambda$\n",
    "        * `t_` is $t$\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of samples\n",
    "        n_samples = x1.shape[0]\n",
    "        # $t$ tensor\n",
    "        t = torch.full((n_samples,), t_, device=self.device)\n",
    "        # $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n",
    "        xt = (1 - lambda_) * self.diffusion.q_sample(x1, t) + lambda_ * self.diffusion.q_sample(x2, t)\n",
    "\n",
    "        # $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n",
    "        return self._sample_x0(xt, t_)\n",
    "\n",
    "    def interpolate_animate(self, x1: torch.Tensor, x2: torch.Tensor, n_frames: int = 100, t_: int = 100,\n",
    "                            create_video=True):\n",
    "        \"\"\"\n",
    "        #### Interpolate two images $x_0$ and $x'_0$ and make a video\n",
    "        * `x1` is $x_0$\n",
    "        * `x2` is $x'_0$\n",
    "        * `n_frames` is the number of frames for the image\n",
    "        * `t_` is $t$\n",
    "        * `create_video` specifies whether to make a video or to show each frame\n",
    "        \"\"\"\n",
    "\n",
    "        # Show original images\n",
    "        self.show_image(x1, \"x1\")\n",
    "        self.show_image(x2, \"x2\")\n",
    "        # Add batch dimension\n",
    "        x1 = x1[None, :, :, :]\n",
    "        x2 = x2[None, :, :, :]\n",
    "        # $t$ tensor\n",
    "        t = torch.full((1,), t_, device=self.device)\n",
    "        # $x_t \\sim q(x_t|x_0)$\n",
    "        x1t = self.diffusion.q_sample(x1, t)\n",
    "        # $x'_t \\sim q(x'_t|x_0)$\n",
    "        x2t = self.diffusion.q_sample(x2, t)\n",
    "\n",
    "        frames = []\n",
    "        # Get frames with different $\\lambda$\n",
    "        for i in monit.iterate('Interpolate', n_frames + 1, is_children_silent=True):\n",
    "            # $\\lambda$\n",
    "            lambda_ = i / n_frames\n",
    "            # $$\\bar{x}_t = (1 - \\lambda)x_t + \\lambda x'_0$$\n",
    "            xt = (1 - lambda_) * x1t + lambda_ * x2t\n",
    "            # $$\\bar{x}_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|\\bar{x}_t)$$\n",
    "            x0 = self._sample_x0(xt, t_)\n",
    "            # Add to frames\n",
    "            frames.append(x0[0])\n",
    "            # Show frame\n",
    "            if not create_video:\n",
    "                self.show_image(x0[0], f\"{lambda_ :.2f}\")\n",
    "\n",
    "        # Make video\n",
    "        if create_video:\n",
    "            self.make_video(frames)\n",
    "\n",
    "    def _sample_x0(self, xt: torch.Tensor, n_steps: int):\n",
    "        \"\"\"\n",
    "        #### Sample an image using $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "        * `xt` is $x_t$\n",
    "        * `n_steps` is $t$\n",
    "        \"\"\"\n",
    "\n",
    "        # Number of sampels\n",
    "        n_samples = xt.shape[0]\n",
    "        # Iterate until $t$ steps\n",
    "        for t_ in monit.iterate('Denoise', n_steps):\n",
    "            t = n_steps - t_ - 1\n",
    "            # Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "            xt = self.diffusion.p_sample(xt, xt.new_full((n_samples,), t, dtype=torch.long))\n",
    "\n",
    "        # Return $x_0$\n",
    "        return xt\n",
    "\n",
    "    def sample(self, n_samples: int = 16):\n",
    "        \"\"\"\n",
    "        #### Generate images\n",
    "        \"\"\"\n",
    "        # $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n",
    "        xt = torch.randn([n_samples, self.image_channels, self.image_size, self.image_size], device=self.device)\n",
    "\n",
    "        # $$x_0 \\sim \\textcolor{lightgreen}{p_\\theta}(x_0|x_t)$$\n",
    "        x0 = self._sample_x0(xt, self.n_steps)\n",
    "        torch.save(x0,\"model_output.pt\")\n",
    "        # Show images\n",
    "        path = './result_image'\n",
    "        isExists = os.path.exists(path)\n",
    "        if not isExists:\n",
    "            os.makedirs(path)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # torch.save(x0,\"model_output.pt\")\n",
    "            # self.show_image(x0[i])\n",
    "            self.show_image(x0[i],title = str(i))\n",
    "\n",
    "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor, eps_theta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        #### Sample from $\\textcolor{lightgreen}{p_\\theta}(x_{t-1}|x_t)$\n",
    "        \\begin{align}\n",
    "        \\textcolor{lightgreen}{p_\\theta}(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n",
    "        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n",
    "        \\textcolor{lightgreen}{\\mu_\\theta}(x_t, t)\n",
    "          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
    "            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "        # [gather](utils.html) $\\bar\\alpha_t$\n",
    "        alpha_bar = gather(self.alpha_bar, t)\n",
    "        # $\\alpha_t$\n",
    "        alpha = gather(self.alpha, t)\n",
    "        # $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n",
    "        eps_coef = (1 - alpha) / (1 - alpha_bar) ** .5\n",
    "        # $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
    "        #      \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n",
    "        mean = 1 / (alpha ** 0.5) * (xt - eps_coef * eps_theta)\n",
    "        # $\\sigma^2$\n",
    "        var = gather(self.sigma2, t)\n",
    "\n",
    "        # $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "        eps = torch.randn(xt.shape, device=xt.device)\n",
    "        # Sample\n",
    "        return mean + (var ** .5) * eps\n",
    "\n",
    "    def p_x0(self, xt: torch.Tensor, t: torch.Tensor, eps: torch.Tensor):\n",
    "        \"\"\"\n",
    "        #### Estimate $x_0$\n",
    "        $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n",
    "         \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n",
    "        \"\"\"\n",
    "        # [gather](utils.html) $\\bar\\alpha_t$\n",
    "        alpha_bar = gather(self.alpha_bar, t)\n",
    "\n",
    "        # $$x_0 \\approx \\hat{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha}}\n",
    "        #  \\Big( x_t - \\sqrt{1 - \\bar\\alpha_t} \\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t) \\Big)$$\n",
    "        return (xt - (1 - alpha_bar) ** 0.5 * eps) / (alpha_bar ** 0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Configs(BaseConfigs):\n",
    "    \"\"\"\n",
    "    Class for holding configuration parameters for training a DDPM model.\n",
    "\n",
    "    Attributes:\n",
    "        device (torch.device):           Device on which to run the model.\n",
    "        show (bool):                     Extract model information.\n",
    "        eps_model (UNet):                U-Net model for the function `epsilon_theta`.\n",
    "        diffusion (DenoiseDiffusion):    DDPM algorithm.\n",
    "        image_channels (int):            Number of channels in the image (e.g. 3 for RGB).\n",
    "        image_size (int):                Size of the image.\n",
    "        n_channels (int):                Number of channels in the initial feature map.\n",
    "        epochs (int):                    Number of training epochs.\n",
    "        batch_size (int):                Batch size.\n",
    "        clip (float):                    Magnitude of maximal gradients allowed.\n",
    "        dropout (float):                 The probability for the dropout of units.\n",
    "        channel_multipliers (List[int]): Number of channels at each resolution.\n",
    "        is_attention (List[bool]):       Indicates whether to use attention at each resolution.\n",
    "        convolutional_block (str):       Type of the convolutional block used\n",
    "        n_steps (int):                   Number of time steps.\n",
    "        n_samples (int):                 Number of samples to generate.\n",
    "        learning_rate (float):           Learning rate.\n",
    "        dataset (torch.utils.data.Dataset):         Dataset to be used for training.\n",
    "        data_loader (torch.utils.data.DataLoader):  DataLoader for loading the data for training.\n",
    "        optimizer (torch.optim.Adam):               Optimizer for the model.\n",
    "    \"\"\"\n",
    "    # Device to train the model on.\n",
    "    # [`DeviceConfigs`\n",
    "    #  picks up an available CUDA device or defaults to CPU.\n",
    "    device: torch.device = DeviceConfigs()\n",
    "    # Retrieve model information\n",
    "    show: bool = True\n",
    "\n",
    "    # U-Net model for $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$\n",
    "    eps_model: UNet\n",
    "    # [DDPM algorithm](index.html)\n",
    "    diffusion: DenoiseDiffusion\n",
    "\n",
    "    # Number of channels in the image. $3$ for RGB.\n",
    "    image_channels: int = 3\n",
    "    # Image size\n",
    "    image_size: int = 32\n",
    "    # Number of channels in the initial feature map\n",
    "    n_channels: int = 64  # 64 (Default: Ho et al.; Limit is VRAM)\n",
    "\n",
    "    # Batch size\n",
    "    batch_size: int = 64  # 64 (Default: Ho et al.; Limit is VRAM)\n",
    "    # Number of training epochs\n",
    "    epochs: int = 1000\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate: float = 2e-5\n",
    "    # Set maximal gradient value\n",
    "    clip: float = 1.0\n",
    "    # Set the dropout probability\n",
    "    dropout: float = 0.1\n",
    "    # The list of channel numbers at each resolution.\n",
    "    # The number of channels is `channel_multipliers[i] * n_channels`\n",
    "    channel_multipliers: List[int] = [1, 2, 2, 4]\n",
    "    # The list of booleans that indicate whether to use attention at each resolution\n",
    "    is_attention: List[int] = [False, False, False, True]\n",
    "    # Convolutional block type used in the UNet blocks. Possible options are 'residual' and 'recurrent'.\n",
    "    convolutional_block: str = 'recurrent'\n",
    "\n",
    "    # Number of time steps $T$ (with $T$ = 1_000 from Ho et al).\n",
    "    n_steps: int = 1000  # 1000 (Default: Ho et al.)\n",
    "    # Number of samples to generate\n",
    "    n_samples: int = 16\n",
    "\n",
    "    # Dataset\n",
    "    dataset: torch.utils.data.Dataset\n",
    "    # Dataloader\n",
    "    data_loader: torch.utils.data.DataLoader\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer: torch.optim.Adam\n",
    "\n",
    "    def init(self):\n",
    "        \"\"\"\n",
    "        Initialize the model, dataset, and optimizer objects.\n",
    "        \"\"\"\n",
    "        # Create εθ(x_t, t) model\n",
    "        self.eps_model = UNet(\n",
    "            image_channels=self.image_channels,\n",
    "            n_channels=self.n_channels,\n",
    "            ch_mults=self.channel_multipliers,\n",
    "            dropout=self.dropout,\n",
    "            is_attn=self.is_attention,\n",
    "            conv_block=self.convolutional_block\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Create [DDPM class]\n",
    "        self.diffusion = DenoiseDiffusion(\n",
    "            eps_model=self.eps_model,\n",
    "            n_steps=self.n_steps,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        # Show the number of params used by the model\n",
    "        if self.show:\n",
    "            pytorch_total_params = sum(p.numel() for p in self.eps_model.parameters())\n",
    "            print(f'The total number of parameters are: {pytorch_total_params}')\n",
    "        # Data augmentation (skipped, due to runtime)\n",
    "        # self.augment()\n",
    "        # Create dataloader\n",
    "        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n",
    "        # Create optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Image logging\n",
    "        tracker.set_image(\"sample\", True)\n",
    "\n",
    "    def sample(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate samples from a trained Denoising Diffusion Probabilistic Model (DDPM).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Sample from the noise distribution at the final time step: x_T ~ p(x_T) = N(x_T; 0, I)\n",
    "            x = torch.randn([self.n_samples, self.image_channels, self.image_size, self.image_size],\n",
    "                            device=self.device)\n",
    "\n",
    "            # Remove noise at each time step in reverse order (so remove noise for T steps)\n",
    "            for t_ in monit.iterate('Sample', self.n_steps):\n",
    "                # Get current time step\n",
    "                t = self.n_steps - t_ - 1\n",
    "                # Sample from the noise distribution at the current time step: x_{t-1} ~ p_theta(x_{t-1}|x_t)\n",
    "                x = self.diffusion.p_sample(x, x.new_full((self.n_samples,), t, dtype=torch.long))\n",
    "\n",
    "            # Log the final denoised samples\n",
    "            tracker.save('sample', x)\n",
    "\n",
    "    def augment(self):\n",
    "        \"\"\"\n",
    "        Augment the data set with color jittering (brithness, contrast, saturation, and hue) and\n",
    "        affine transformations (ratation, translation, scaling and shearing). Used to increase the data set size,\n",
    "        making the predictions more robust to attain view point invariance.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.dataset.data.shape) == 3:\n",
    "            self.dataset.data = self.dataset.data[:, None, :, :]\n",
    "\n",
    "        transformations = [T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "                           T.RandomAffine(degrees=90, translate=(0, 0.5), scale=(0.5, 0.5), shear=(0, 0.5))]\n",
    "\n",
    "        for transform in transformations:\n",
    "            img_trans = transform(self.dataset.data)\n",
    "            self.dataset.data = torch.cat((img_trans, self.dataset.data), dim=0)\n",
    "            self.dataset.targets = torch.cat((self.dataset.targets, self.dataset.targets), dim=0)\n",
    "        self.dataset.data = torch.squeeze(self.dataset.data)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Train a Denoising Diffusion Probabilistic Model (DDPM) with the set dataloader.\n",
    "        \"\"\"\n",
    "        data_steps = 0\n",
    "        curr_loss = 0\n",
    "        # Iterate through the dataset\n",
    "        for data in monit.iterate('Train', self.data_loader):\n",
    "            # Increment global step\n",
    "            tracker.add_global_step()\n",
    "            # Move data to device\n",
    "            data = data.to(self.device)\n",
    "\n",
    "            # Make the gradients zero\n",
    "            self.optimizer.zero_grad()\n",
    "            # Calculate loss\n",
    "            loss = self.diffusion.loss(data)\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            # Clip model gradients\n",
    "            # clip_grad_value_(parameters=self.eps_model.parameters(), clip_value=self.clip)\n",
    "            # Take an optimization step\n",
    "            self.optimizer.step()\n",
    "            # Track the loss\n",
    "            tracker.save('loss', loss)\n",
    "            curr_loss+=loss.item()\n",
    "            data_steps+=1\n",
    "        print(f\"Loss after {data_steps} input data seen: {round(curr_loss,2)}\")\n",
    "        dirs = 'loss_log_'+\"recurrent\"+'mnist'+'.txt'\n",
    "\n",
    "        with open(dirs, 'a', ) as loss_log_file:\n",
    "            loss_info = \"{}, {}\".format(data_steps, curr_loss)\n",
    "            loss_log_file.write(loss_info+'\\n')\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        ### Training loop\n",
    "        \"\"\"\n",
    "        for _ in monit.loop(self.epochs):\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            # Sample some images\n",
    "            # self.sample()\n",
    "            # New line in the console\n",
    "            tracker.new_line()\n",
    "            # Save the model\n",
    "            experiment.save_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate samples\"\"\"\n",
    "\n",
    "    # Training experiment run UUID\n",
    "    run_uuid = \"new_recurrent_mnist\"\n",
    "\n",
    "    # Start an evaluation\n",
    "    experiment.evaluate()\n",
    "\n",
    "    # Create configs\n",
    "    configs = Configs()\n",
    "    # Load custom configuration of the training run\n",
    "    configs_dict = experiment.load_configs(run_uuid)\n",
    "    # Set configurations\n",
    "    experiment.configs(configs, configs_dict)\n",
    "\n",
    "    # NOTE: This needs to be remembered manually it seems...\n",
    "\n",
    "    # Set configurations. You can override the defaults by passing the values in the dictionary.\n",
    "    experiment.configs(configs, {\n",
    "        'dataset': 'MNIST',  # 'CIFAR10', 'CelebA' 'MNIST'\n",
    "        'image_channels': 1,  # 3, 3, 1\n",
    "        'epochs': 5,  # 100, 100, 5\n",
    "    })\n",
    "\n",
    "    # Initialize\n",
    "    configs.init()\n",
    "\n",
    "    # Set PyTorch modules for saving and loading\n",
    "    experiment.add_pytorch_models({'eps_model': configs.eps_model})\n",
    "\n",
    "    # Load training experiment\n",
    "    experiment.load(run_uuid=run_uuid, checkpoint=64906)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = Sampler(diffusion=configs.diffusion,\n",
    "                      image_channels=configs.image_channels,\n",
    "                      image_size=configs.image_size,\n",
    "                      device=configs.device)\n",
    "\n",
    "    # Start evaluation\n",
    "    with experiment.start():\n",
    "        # No gradients\n",
    "        with torch.no_grad():\n",
    "            # Sample an image with an denoising animation\n",
    "            sampler.sample_animation()\n",
    "            sampler.sample(n_samples = 2)\n",
    "            if False:\n",
    "                # Get some images fro data\n",
    "                data = next(iter(configs.data_loader)).to(configs.device)\n",
    "\n",
    "                # Create an interpolation animation\n",
    "                sampler.interpolate_animate(data[0], data[1])\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec2ec5fed0de0c49a37c3ddbe53ace80a23696fd397f90ad02c586241806f448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
